---
title: "Random Samples, Central Limit Theorem, and Estimators"
author: "Hadijat Oke 117818512, Thomas Urdinola 117971262"
output:
  html_document:
    number_sections: yes
    toc: yes  
  pdf_document:
    toc: yes
---

In this project, we will work with the sampling distributions of statistics, the Central Limit Theorem (CLT), and properties of estimators.  


The Central Limit Theorem and the properties of estimators are one of the most powerful concepts/tools that we learn in this course. It will send you back to the beginning of this class, require you to remind yourself and use the distributions and probability principles that we studied then, and yet at the end of this class now, it will set the stage for new beginnings for many things to come. Make sure to have fun, and good luck!

# Random Samples
## Binomial Distibution
Consider the Binomial distribution $\text{Binom}(n=10, p = 0.8)$. We will get random samples from this distribution and check out the histograms for each of these samples. We expect that as the sample size increases, the sample distribution will provide an increasingly improved approximation of the the true distribution. 

### Probability Mass Function of $\text{Binom}(n=10, p = 0.8)$..
Plot the probability mass function of $\text{Binom}(n=10, p = 0.8)$.
```{r}
# Load necessary library for plotting
library(ggplot2)

# Define parameters
n <- 10
p <- 0.8

# Create a sequence of x values from 0 to n
x_values <- 0:n

# Calculate the PMF using the dbinom function
pmf_values <- dbinom(x_values, size = n, prob = p)

# Create a data frame for plotting
data <- data.frame(x = x_values, pmf = pmf_values)

# Plot the PMF
ggplot(data, aes(x = x, y = pmf)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "PMF of Binomial Distribution (n=10, p=0.8)",
       x = "Number of Successes",
       y = "Probability") +
  theme_minimal()

```

### Sample distributions
For the sample sizes $4, 7, 10, 15, 20, 30, 40, 80, 1000$ get random samples of respective size, and plot the histograms for each of these samples. 

```{r}
# Sample sizes
sample_sizes <- c(4, 7, 10, 15, 20, 30, 40, 80, 1000)

# Generate random samples and plot histograms for each sample size
par(mfrow = c(3, 3))  # Set the layout of the plots

for (i in 1:length(sample_sizes)) {
  sample_size <- sample_sizes[i]
  sample <- rbinom(sample_size, size = 10, prob = 0.8)  # Generating random samples from a Binomial distribution
  
  hist(sample, main = paste("Sample Size:", sample_size), xlab = "Number of Successes", col = "skyblue", border = "black")
}
```
WHAT DO YOU NOTICE?
When the sample size increases, the histograms tend to shape themselves to a more
bell-shaped curve. This could be attributed to the central limit theorem


## Gamma Distribution
Consider the Gamma distribution $\text{Gamma}(\alpha = 2, \beta = 1.5)$. We will get random samples from this distribution and check out the histograms for each of these samples. We expect that as the sample size increases, the sample distribution will provide an increasingly improved approximation of the the true distribution. 

### Density of $\text{Gamma}(\alpha = 4, \beta = 2)$.
Plot the density curve of $\text{Gamma}(\alpha = 4, \beta = 2)$,
```{r}
# Load necessary library for plotting
library(ggplot2)

# Define parameters
alpha <- 4
beta <- 2

# Generate a sequence of x values
x_values <- seq(0, 20, by = 0.1)  # Change the range and granularity as needed

# Calculate the density values using dgamma function
density_values <- dgamma(x_values, shape = alpha, rate = beta)

# Create a data frame for plotting
data <- data.frame(x = x_values, density = density_values)

# Plot the density curve
ggplot(data, aes(x = x, y = density)) +
  geom_line(color = "blue", linewidth = 1) +
  labs(title = "Density Curve of Gamma Distribution",
       x = "x",
       y = "Density") +
  theme_minimal()


```

### Sample distributions
For the sample sizes $4, 7, 10, 15, 20, 30, 40, 80, 1000$ get random samples of respective size, and plot the histograms for each of these samples. 

```{r}
# Sample sizes
sample_sizes <- c(4, 7, 10, 15, 20, 30, 40, 80, 1000)

# Generate random samples and plot histograms for each sample size
par(mfrow = c(3, 3))  # Set the layout of the plots

for (i in 1:length(sample_sizes)) {
  sample_size <- sample_sizes[i]
  sample <- rgamma(sample_size, shape = 4, rate = 2)  # Generating random samples from a Gamma distribution
  
  hist(sample, main = paste("Sample Size:", sample_size), xlab = "Value", col = "skyblue", border = "black")
}

```
What do you notice?
- The same pattern as above, higher sample sizes tend to shape themselves around
a central mean, thus creating a bell shaped curve

# Sampling Distribution of Sample Max
## Discrete Uniform distribution
Suppose we are working with the discrete uniform random variable taking values $\{1, 2, 3, 4, 5, 6\}$.

Plot the pmf of the given discrete distribution
```{r}
# Define the values of the discrete uniform random variable
values <- 1:6

# Calculate the probability mass function (since it's uniform, each value has equal probability)
prob <- rep(1/length(values), length(values))

# Create a data frame for plotting
data <- data.frame(values = values, probability = prob)

# Plot the PMF
library(ggplot2)

ggplot(data, aes(x = as.factor(values), y = probability)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "PMF of Discrete Uniform Distribution",
       x = "Values",
       y = "Probability") +
  theme_minimal() +
  scale_x_discrete(labels = as.character(values))

```



Define a function "disc_samp" that takes input "n" and returns a random sample of size "n" from this distribution.
```{r}
# Define the values of the discrete random variable
values <- 1:6

# Function to generate a random sample from a discrete uniform distribution
disc_samp <- function(n) {
  sample(values, size = n, replace = TRUE)
}

```
Use the "disc_samp" function and the replicate function to get the histograms for the sampling distribution of the sample max when working with sample sizes $n = 1, 2, 3, 4,5, 15$.
Be sure to have appropriate titles for your histograms.

```{r}
# Define the function to generate the sample maximum
sample_max <- function(n) {
  max(disc_samp(n))
}

# Sample sizes
sample_sizes <- c(1, 2, 3, 4, 5, 15)

# Generate the sampling distribution of sample maxima and plot histograms
par(mfrow = c(2, 3))  # Set the layout of the plots

for (size in sample_sizes) {
  max_samples <- replicate(1000, sample_max(size))  # Generating 1000 samples
  
  hist(max_samples, main = paste("Sample Size:", size),
       xlab = "Maximum Value", col = "skyblue", border = "black")
}

```
WHAT DO YOU NOTICE
Larger sample sizes can attribute to less variety in answers when trying to find
the sample max. The main reason is because when you have more numbers to be part
of the sample, we could have a higher chance of rolling bigger numbers.

## A Skewed Discrete Distribution
Suppose we are working with the discrete  random variable taking values $\{1, 2, 3, 4, 5, 6\}$ and having the following probability mass function: $$p(1) = 0.5, \quad  p(2) = p(3) = \cdots = p(6) = 0.1$$

Plot the pmf of the given discrete distribution
```{r}
# Define the values of the discrete random variable
values <- 1:6

# Define probabilities for each value
probabilities <- c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1)

# Create a data frame for plotting
data <- data.frame(values = values, probability = probabilities)

# Plot the PMF
library(ggplot2)

ggplot(data, aes(x = as.factor(values), y = probability)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "PMF of Discrete Distribution",
       x = "Values",
       y = "Probability") +
  theme_minimal() +
  scale_x_discrete(labels = as.character(values))

```
Define a function "disc_samp_1" that takes input "n" and returns a random sample of size "n" from this distribution.
```{r}
# Define the values of the discrete random variable
values <- 1:6

# Define probabilities for each value
probabilities <- c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1)

# Function to generate a random sample from the given discrete distribution
disc_samp_1 <- function(n) {
  sample(values, size = n, replace = TRUE, prob = probabilities)
}
 
```
Use the "disc_samp_1" function and the replicate function to get the histograms for the sampling distribution of the sample max when working with sample sizes $n = 1, 2, 3, 4,5 15$.
Be sure to have appropriate titles for your histograms.

```{r}
# Define the function to generate the sample maximum
sample_max <- function(n) {
  max(disc_samp_1(n))
}

# Sample sizes
sample_sizes <- c(1, 2, 3, 4, 5, 15)

# Set seed for reproducibility
set.seed(42)

# Generate the sampling distribution of sample maxima and plot histograms
par(mfrow = c(2, 3))  # Set the layout of the plots

for (size in sample_sizes) {
  max_samples <- replicate(1000, sample_max(size))  # Generating 1000 samples
  
  hist(max_samples, main = paste("Sample Size:", size),
       xlab = "Maximum Value", col = "skyblue", border = "black")
}

```
WHAT DO YOU NOTICE
Higher sample sizes leads to lower variety in maximum values. It looks like it's
skewed to the left every time we increase the size

# Central Limit Theorem
## Exponential Distribution
Suppose we are working with a population that has the exponential distibution with $\lambda = 2$. 

Use the replicate function to get the histograms for the sampling distribution of the sample mean when working with sample sizes $n = 1, 2, 3, 4, 15, 500$.
Be sure to have appropriate titles for your histograms. 

```{r}
samp_sizes  <- c(1, 2, 3, 4, 15, 500, 1000)

par(mfrow=c(2,2))
for(size in samp_sizes){
  replicates <- replicate(10000, {
    mean(rexp(size, rate = 5))
  })
  hist(replicates, breaks = 100,
       main = paste("Samp Dist for Exp, samp size =", size ))
}
```

What do you notice?
Larger sample sizes leads to a more normal bell-curve distribution

## Discrete Uniform distibution

Suppose we are working with the discrete uniform random variable taking values $\{1, 2, 3, 4, 5, 6\}$.

Define a function "disc_samp" that takes input "n" and returns a random sample of size "n" from this distribution.

```{r}


disc_samp <- function(n) {
  sample(1:6, n, replace=T)
}



```

Use the "disc_samp" function and the replicate function to to get the histograms for the sampling distribution of the sample mean when working with sample sizes $n = 1, 2, 3, 4, 15, 500$.
Be sure to have appropriate titles for your histograms. 
```{r}


# Define the function to generate the sample maximum
sample_mean <- function(n) {
  mean(disc_samp(n))
}

# Sample sizes
sample_sizes <- c(1, 2, 3, 4, 15, 500)

# Set seed for reproducibility
set.seed(42)

# Generate the sampling distribution of sample maxima and plot histograms
par(mfrow = c(2, 3))  # Set the layout of the plots




for (size in sample_sizes) {
  max_samples <- replicate(1000, sample_mean(size))  # Generating 1000 samples
  
  hist(max_samples, main = paste("Sample Size:", size), 
       xlab = "Maximum Value", col = "skyblue", border = "black")
}


```

What do you notice?

As the sample size increases the distribution of the sample mean becomes more normal. 

## Continuous Uniform distibution

Suppose we are working with the Continuous uniform random variable taking values on $(0,1)$.

Define a function "cont_uni_samp" that takes input "n" and returns a random sample of size "n" from this distribution.

```{r}

cont_uni_samp <- function(n) {
  sample(0:1, n, replace=T)
}


```

Use the "cont_uni_samp" function and the replicate function to to get the histograms for the sampling distribution of the sample mean when working with sample sizes $n = 1, 2, 3, 4, 15, 500$.
Be sure to have appropriate titles for your histograms. 
```{r}

# Define the function to generate the sample maximum
sample_mean <- function(n) {
  mean(cont_uni_samp(n))
}

# Sample sizes
sample_sizes <- c(1, 2, 3, 4, 15, 500)

# Set seed for reproducibility
set.seed(42)

# Generate the sampling distribution of sample maxima and plot histograms
par(mfrow = c(2, 3))  # Set the layout of the plots




for (size in sample_sizes) {
  max_samples <- replicate(1000, sample_mean(size))  # Generating 1000 samples
  
  hist(max_samples, main = paste("Sample Size:", size), 
       xlab = "Maximum Value", col = "skyblue", border = "black")
}
```

What do you notice?

The frequency of each mean increased from the last distribution but ti still follows the pattern of becoming a normal distribution. 



# Estimators for the Gamma parameters
Recall from class that we calculated the estimators for $\alpha$ and $\beta$ using the method of moments for a sample of data values of size $n$ as:
$$\hat{\alpha} = \frac{\overline{X}^2}{(1/n)\sum X_i^2 - \overline{X}^2}\quad \hat{\beta} = \frac{(1/n)\sum X_i^2 - \overline{X}^2}{\overline{X}}$$ 

Write functions "alpha_hat" and "beta_hat" that take input a sample of data values "samp" and output the given estimators.
```{r}


alpha_hat <- function(samp) {
  
  sum = 0
  len = length(samp)
  mean = mean(samp)

  for(i in samp){
    sum = sum + (i^2) - mean^2
    
  }
  
  return(mean^2/((1/len) * (sum)))
  
}

beta_hat <- function(samp){
  
  sum = 0
  len = length(samp)
  mean = mean(samp)

  for(i in samp){
    sum = sum + (i^2) - mean^2
    
  }
  
  return(((1/len) * (sum))/mean)
  
  
  
}

```
Now let's work with a Gamma distribution with $\alpha = 2$ and $\beta = 4$. 

## Bias, Variance, and MSE of Gamma Estimators.
Recall that we calculated the following identity in class:
$$ \rm{MSE}(\hat{\theta}) = V(\hat{\theta}) + (\rm{Bias}(\hat{theta}))^2.$$ 
This problem will empirically verify this identity. 
\\

Use the replicate function to calculate the empirical variance and bias of the estimators $\hat{\alpha}$ and $\hat{\beta}$ when sampling a sample of size 30 from $\rm{Gamma}(\alpha = 2, \beta = 4)$.
Store these numbers in variables "var_emp" and "bias_emp".

```{r}

alpha_results <- replicate(10000, {
  samp<- rgamma(30,shape = 2, scale = 4)
  alpha_hat(samp)
})

var_emp_a <- var(alpha_results)
bias_emp_a <- mean(alpha_results) - 2

cat("Variance for alpha ",var_emp_a)
cat("\n")
cat("Bias for alpha ", bias_emp_a)
cat("\n")



beta_results <- replicate(10000, {
  samp<- rgamma(30,shape = 2, scale = 4)
  beta_hat(samp)
})

var_emp <- var(beta_results)
bias_emp <- mean(beta_results) - 4

cat("Variance for beta ",var_emp)
cat("\n")
cat("Bias for beta ", bias_emp)



```

Use the replicate function to calculate the Mean Squared Error for $\hat{\alpha}$ and $\hat{\beta}$ when sampling a sample of size 30 from $\rm{Gamma}(\alpha = 2, \beta = 4)$. Store this number in "MSE_emp".
```{r}

alpha_results <- replicate(10000, {
  samp<- rgamma(30,shape = 2, scale = 4)
  alpha_hat(samp)
})



MSE_emp_a <- mean((alpha_results - 2)^2)
cat ("MSE of alpha ", MSE_emp_a)
cat("\n")              



beta_results <- replicate(10000, {
  samp<- rgamma(30,shape = 2, scale = 4)
  beta_hat(samp)
})

MSE_emp <- mean((beta_results - 4)^2)
cat ("MSE of beta ", MSE_emp)
                

```

If there is justice in this world, you must get $$\rm{mse}_{emp}\approx\rm{var}_{emp} + \rm{bias}_{emp}.$$
Check if the above approximation holds. 
```{r}
MSE_emp_a - (var_emp_a + (bias_emp_a))
MSE_emp - (var_emp + bias_emp)





```


# Bias, Variance, and MSE for estimator for $\mu^2$
Suppose we have a random sample of size n coming from the normal distribution $N(\mu= 5, \sigma = 1.5)$. Recall from class that $\hat{\theta} = \overline{X}^2$ is a biased estimator for $\mu^2$, and we calculated the bias to be

$$ \text{Bias}(\hat{\theta}) = \frac{\sigma^2}{n}.$$ 

Write function that takes input a random sample and outputs the square of the sample mean (this is the estimator $\hat{\theta}$. 
```{r}

sample_mean <- function(n) {
  mean(n)^(2)
}




```

Use the replicate function to calculate the empirical variance and bias of the estimator $\hat{\theta}$ when sampling a sample of size 15 from $N(\mu = 5, \sigma = 1.5)$.
Store these numbers in variables "var_emp" and "bias_emp".
```{r}

theta_estimates <- replicate(1000, {
  sample_data <- rnorm(15, mean = 5, sd = 1.5)
  sample_mean(sample_data)
})

# Generate replicates and calculate theta_hat


# Calculate empirical variance and bias for theta_hat
var_emp <- var(theta_estimates)
bias_emp <- mean(theta_estimates) - 5^2

cat("Empirical Variance of theta_hat:", var_emp)
cat("\n")
cat("Empirical Bias of theta_hat:", bias_emp)
cat("\n")



```





Use the replicate function to calculate the Mean Squared Error for $\hat{\theta}$ when sampling a sample of size 15 from  $N(\mu = 5, \sigma = 1.5)$. Store this number in "MSE_emp".

```{r}



# Generate replicates and calculate theta_hat
theta_estimates <- replicate(1000, {
  sample_data <- rnorm(15, mean = 5, sd = 1.5)
  sample_mean(sample_data)
})

# Calculate true value for theta
true_value <- 5^2

# Calculate Mean Squared Error (MSE) for theta_hat
MSE_emp <- mean((theta_estimates - true_value)^2)

cat("Mean Squared Error of theta_hat:", MSE_emp, "\n")

```


You must hopefully get $$\rm{mse}_{emp}\approx\rm{var}_{emp} + \rm{bias}_{emp}^2.$$

Check if this is true for the estimator $\hat{\theta}$. 
```{r}

MSE_emp - (var_emp + (bias_emp)^2)


```